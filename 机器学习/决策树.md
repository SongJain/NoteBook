# 决策树（Decision Tree ）

## 统计学基础

### 比特化

1、随机变量X，设 P(X = A) = 1/4，P(X = B) = 1/4，P(X = C) = 1/4，P(X = D) = 1/4。则此时可以使用两位比特位表示X，因为X的各个值的出现概率相同。此时比特化可以这样实现 A：00，B：01，C：10，D：11。

2、随机变量X，设 P(X = A) = 1/2，P(X = B) = 1/4，P(X = C) = 1/8，P(X = D) = 1/8。则此时平均需要1.75个bits。此时比特化需要这样实现 A：0，B：10，C：110，D：111。

取值 K 需要用多少个bits位来描述：
$$
-\log_{2}{P(X=K)}
$$
3、一般化的比特化

设随机变量 X 具有 m 个值，分别为 V1，V2，V3.....Vm，并且各个值出现的概率：
$$
P(X = V1) = P1 ，P(X = V2) = P3， .....P(X = Vm) = Pm
$$
则可以用变量的期望来表示每个变量需要多少个bits位：
$$
E(X) = -\sum_{i=1}^{m}Pi(X)\log_{2}{Pi(X)}
$$
这里的E(X)就是信息熵。

### 信息熵（Entropy）

高信息熵：表示随机变量X是均匀分布的，等概率出现。

低信息熵：表示随机变量X不是等概率分布的。

``````python
# 计算dataSet的信息熵
def calcShannonEnt(dataSet):
    numEntries = len(dataSet)
    # 创建一个 字典
    labelCounts = {}
    # 整个循环就是在统计data中的 label 的种类和对应种类出现次数
    # 因为信息熵是一个累加的过程，并且每个label都要对应的上
    for featVec in dataSet: #the the number of unique elements and their occurance
        # -1 就是取出最后一列的值
        currentLabel = featVec[-1]
        # 如果label不存在就创建，并且赋值为0
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] = 0
        #如果存在就在对应的 key 上计数+1
        labelCounts[currentLabel] += 1
    #设置信息熵为0，初始化
    shannonEnt = 0.0
    #计算信息熵，根据公式
    for key in labelCounts:
        prob = float(labelCounts[key])/numEntries
        shannonEnt -= prob * log(prob,2) #log base 2
    return shannonEnt
``````

## 决策树概述

通过一系列的非叶子节点对输入进行判断测试，每个节点的分支代表一个测试的输出，每一个叶子节点代表了一个类别。

如图

![](D:\QQPCmgr\Desktop\2.png)

这里假设我们要预测一个人是否有贷款意向，这对某些银行的个性化推荐也有一定作用，当然这里并不全面。首先我们对该人的职业进行测试，如图，测试结果分为四种输出结果：自由职业、白领和工人；接着，我们对该人的年龄进行测试，当然，年龄越小，贷款的‘可能性越小，反之则贷款可能性越大；接下来又有对收入、学历等等的测试。我们可以发现每个叶子节点对应的要么就是“无贷款意向”，要么就是“有贷款意向”，也就是我们预测的类别最多也就两个类别，有或者没有。

决策树分为 分类树 和 回归树。区别在于，分类树是分类标签的值，回归树是预测连续的值。

## 决策树分割属性选择

1、决策树采用贪心策略。只考虑当前数据特征下的最优划分方式，不能回溯。

2、对整体的数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果集的“纯度”进行比较，选择纯度更纯的特征属性作为当前需要分割的数据集进行分割，持续迭代。

## 如何量化纯度？

1、决策树的构建基于样本概率和纯度进行操作，判断是否足够纯。

Gini系数
$$
Gini=1-\sum_{i=1}^{m}{P(i)}^{2}
$$
熵(Entroy)
$$
H(Entroy) = -\sum_{i=1}^{m}P_i(X)\log_{2}{P_i(X)}
$$
错误率
$$
Error=1-max(P(1),(P(2),(P(3)...(P(m))
$$
2、计算出每个特征属性的量化纯度值后使用信息增益度来选择当前数据集的分割属性，若信息增益度值越大，则表示在该特征属会损失的纯度越大，那么该属性就需要被放在决策树的上层。
$$
Gain ：\Delta=H(D)-H(D|A)
$$

## 决策树算法

### ID3算法

经典的决策树算法，使用信息熵和信息增益来进行树的构建，每次选择信息增益最大的特征属性作为分割属性。

优点：构建快、简单。

缺点：ID3不是递增算法；ID3是单变量决策树，属性之间的关系不考虑；抗噪性差；适用于小规模数据集。

### C4.5算法

由ID3改进而来，使用信息增益率来取代ID3的信息增益。
$$
GainRate=\frac{Gain(A)}{H(A)}
$$
会进行减枝操作进行优化，自动完成对连续变量的离散化处理。

优点：准确、简单。

缺点：效率低、小规模。

### CART算法

流程相同，只是采用的是Gini系数来构建树。

### 最优特征选择的python实现

```python
# 分割数据，生成子数据集
def splitDataSet(dataSet, axis, value):
    retDataSet = []
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet

#选择最好的特征（信息增益最大）
def chooseBestFeatureToSplit(dataSet):
    # 最后一列是标签，除去标签
    numFeatures = len(dataSet[0]) - 1
    # 计算一开始的信息熵（因为等会要逐个作差）
    baseEntropy = calcShannonEnt(dataSet)
    # 设信息增益为0
    bestInfoGain = 0.0; bestFeature = -1
    # 循环
    for i in range(numFeatures):
        # 给所有样本创建一个List
        featList = [example[i] for example in dataSet]
        # 给不同的特征值建立一个set
        uniqueVals = set(featList)
        # 设置新的信息熵为0
        newEntropy = 0.0
        for value in uniqueVals:
            # 根据不同列来划分数据集，得到一个子数据集
            subDataSet = splitDataSet(dataSet, i, value)
            # 求概率
            prob = len(subDataSet)/float(len(dataSet))
            # 求信息熵，求和（这里每一个特征都要求，因为信息增益是整体的！）
            newEntropy += prob * calcShannonEnt(subDataSet)
        # 信息增益是原始信息熵 - 更新后的信息熵
        infoGain = baseEntropy - newEntropy
        # 选择最信息增益最大的该特征
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
```



## 案例

```python
#_*_coding:utf-8_*_
import numpy as np
import matplotlib as mlb
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# 定义一些常量 因为没有给数据的头部
iris_feature_E = 'sepal length', 'sepal width', 'petal length', 'petal width'
iris_feature_C = '花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度'
iris_class = 'Iris-setosa', 'Iris-versicolor', 'Iris-virginica'

# 读取 data 文件
# 这里 data 没有头部，注意 header 要写为 None
# 这里给 data 添加头部，因此要使用 names 参数，传递每一列的属性名称（标签名称）
path = "C:\\Users\\JAIN\\PycharmProjects\\MachineLearning\\Decision Tree\\iris.data"
name = ['sepal length', 'sepal width', 'petal length', 'petal width','class']
data = pd.read_csv(path,header=None,names=name)

# 读数据 相关数据
# print(data.head(5)) # 查看前5条数据
# print(data.info()) # 查看数据的相关信息
# print(data.describe()) # 查看数据均值、标准差、分布情况信息

# 从原始数据中获取 X Y
# 这里使用了name
X = data[name[:-1]] # 获取前四列数据
Y = data[name[-1]] # 获取最后一列数据
Y = pd.Categorical(Y).codes # 将类别转换为数字

# 数据的划分
X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=18, train_size=0.7)
print("训练集样本的数量:%d" %X_train.shape[0])
print("样本集样本的数量:%d" %X_test.shape[0])

# 模型的训练 使用决策树相关模型进行分类
# sklearn 默认都是CART树模型
"""              criterion="gini",
                 splitter="best",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features=None,
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 class_weight=None,
                 presort=False):"""
tree = DecisionTreeClassifier(criterion="gini", max_depth=7)
tree.fit(X_train, y_train)

# 模型相关的指标输出
print("训练集上的准确率：%3f" % tree.score(X_train,y_train))
print("测试集上的准确率：%3f" % tree.score(X_test,y_test))

# y_hat是预测值
y_hat = tree.predict(X_test)
print("测试集上的准确率：%3f" % (np.mean(y_test == y_hat)))


# 模型的预测
print("测试集预测值：")
print(tree.predict(X_test))
print("测试集各个样本的预测概率：")
print(tree.predict_proba(X_test))

# 在构建决策树时候，是把较重要的节点放到上面的，越往上的节点作用越强
print("各个特征重要性指标：",end="")
print(tree.feature_importances_)
```
